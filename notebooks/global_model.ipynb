{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "502cfb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: backend/forecast_service.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "import joblib\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3762fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional SHAP import check\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17edf6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1) Data loader: M5 -> long panel\n",
    "# -------------------------\n",
    "def load_m5_long(m5_dir: str, store_ids: Optional[List[str]] = None, sku_ids: Optional[List[str]] = None,\n",
    "                 max_items: int = 1000, use_validation: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load M5 sales -> long format and merge calendar + prices.\n",
    "    Returns DataFrame with columns: date, id, item_id, dept_id, cat_id, store_id, state_id, demand, price, calendar...\n",
    "    \"\"\"\n",
    "    m5_dir = Path(m5_dir)\n",
    "    sales_file = \"sales_train_validation.csv\" if use_validation else \"sales_train_evaluation.csv\"\n",
    "    sales = pd.read_csv(m5_dir / sales_file)\n",
    "    cal = pd.read_csv(m5_dir / \"calendar.csv\")\n",
    "    prices = pd.read_csv(m5_dir / \"sell_prices.csv\")\n",
    "\n",
    "    # Optionally filter stores or specific SKUs to limit size\n",
    "    if store_ids is not None:\n",
    "        sales = sales[sales[\"store_id\"].isin(store_ids)].copy()\n",
    "\n",
    "    if sku_ids is not None:\n",
    "        sales = sales[sales[\"id\"].isin(sku_ids)].copy()\n",
    "    else:\n",
    "        sales = sales.head(max_items).copy()\n",
    "\n",
    "    d_cols = [c for c in sales.columns if c.startswith(\"d_\")]\n",
    "    id_cols = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "\n",
    "    long = sales[id_cols + d_cols].melt(\n",
    "        id_vars=id_cols,\n",
    "        value_vars=d_cols,\n",
    "        var_name=\"d\",\n",
    "        value_name=\"demand\",\n",
    "    )\n",
    "\n",
    "    # merge calendar (d -> date + events etc.)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    long = long.merge(cal, on=\"d\", how=\"left\")\n",
    "\n",
    "    # merge price using (store_id, item_id, wm_yr_wk)\n",
    "    long = long.merge(prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how=\"left\")\n",
    "    long = long.rename(columns={\"sell_price\": \"price\"})\n",
    "    long[\"demand\"] = long[\"demand\"].astype(float)\n",
    "    long = long.sort_values([\"id\", \"date\"]).reset_index(drop=True)\n",
    "    return long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d37b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2) Feature engineering per-SKU panel\n",
    "# -------------------------\n",
    "def add_panel_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds lag features, rolling stats, calendar encodings, SNAP, price-driven promo flag.\n",
    "    Input: df with columns ['id','date','demand','price', calendar cols...]\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([\"id\", \"date\"])\n",
    "\n",
    "    # date parts\n",
    "    df[\"dow\"] = df[\"date\"].dt.dayofweek\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    # wday exists in calendar.csv as numeric day-of-week starting at 1, but use dow for model\n",
    "\n",
    "    # event flags\n",
    "    df[\"has_event_1\"] = df[\"event_name_1\"].notna().astype(int)\n",
    "    df[\"has_event_2\"] = df[\"event_name_2\"].notna().astype(int)\n",
    "    # snap flags\n",
    "    for c in [\"snap_CA\", \"snap_TX\", \"snap_WI\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(0).astype(int)\n",
    "\n",
    "    # price features\n",
    "    df[\"price\"] = df[\"price\"].astype(float)\n",
    "    df[\"price_lag_1\"] = df.groupby(\"id\")[\"price\"].shift(1)\n",
    "    df[\"price_change_1\"] = (df[\"price\"] / df[\"price_lag_1\"] - 1).replace([np.inf, -np.inf], np.nan)\n",
    "    # promo flag: price below rolling median of last 28 days (exclude today)\n",
    "    df[\"price_rolling_med_28\"] = df.groupby(\"id\")[\"price\"].shift(1).rolling(28).median().reset_index(level=0, drop=True)\n",
    "    df[\"promo_flag\"] = ((df[\"price\"] < df[\"price_rolling_med_28\"]).astype(float)).fillna(0.0)\n",
    "\n",
    "    # demand lags & rolling stats\n",
    "    for lag in [1, 7, 14, 28]:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(\"id\")[\"demand\"].shift(lag)\n",
    "\n",
    "    df[\"roll_mean_7\"] = df.groupby(\"id\")[\"demand\"].shift(1).rolling(7).mean().reset_index(level=0, drop=True)\n",
    "    df[\"roll_std_7\"]  = df.groupby(\"id\")[\"demand\"].shift(1).rolling(7).std().reset_index(level=0, drop=True)\n",
    "    df[\"roll_mean_28\"]= df.groupby(\"id\")[\"demand\"].shift(1).rolling(28).mean().reset_index(level=0, drop=True)\n",
    "    df[\"roll_std_28\"] = df.groupby(\"id\")[\"demand\"].shift(1).rolling(28).std().reset_index(level=0, drop=True)\n",
    "\n",
    "    # fill some small NaNs created by price features\n",
    "    df[\"price_change_1\"] = df[\"price_change_1\"].fillna(0.0)\n",
    "    df[\"promo_flag\"] = df[\"promo_flag\"].fillna(0.0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b8afa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3) Training service (global model)\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class GlobalForecastModel:\n",
    "    model_mean: Any = None\n",
    "    model_q50: Any = None\n",
    "    model_q90: Any = None\n",
    "    feature_cols: List[str] = None\n",
    "    cat_cols: List[str] = None\n",
    "    last_train_date: Optional[pd.Timestamp] = None\n",
    "    train_metrics: Dict[str, Any] = None\n",
    "\n",
    "    def train(self, df_long: pd.DataFrame, val_days: int = 28, lgb_params: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Train global LightGBM mean + quantile models.\n",
    "        df_long: output of load_m5_long()\n",
    "        \"\"\"\n",
    "        t0 = time.time()\n",
    "        df = df_long.copy()\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df = add_panel_features(df)\n",
    "\n",
    "        # choose feature list (drop ones that don't exist)\n",
    "        cat_cols = [c for c in [\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"] if c in df.columns]\n",
    "        base_cols = [\n",
    "            \"dow\",\"month\",\"year\",\n",
    "            \"has_event_1\",\"has_event_2\",\n",
    "            \"snap_CA\",\"snap_TX\",\"snap_WI\",\n",
    "            \"price\",\"price_change_1\",\"promo_flag\",\n",
    "            \"lag_1\",\"lag_7\",\"lag_14\",\"lag_28\",\n",
    "            \"roll_mean_7\",\"roll_std_7\",\"roll_mean_28\",\"roll_std_28\"\n",
    "        ]\n",
    "        base_cols = [c for c in base_cols if c in df.columns]\n",
    "\n",
    "        # drop rows with missing features (necessary for initial training)\n",
    "        df_model = df.dropna(subset=base_cols).copy()\n",
    "\n",
    "        # time split (global): last val_days days as validation\n",
    "        global_max_date = df_model[\"date\"].max()\n",
    "        split_date = global_max_date - pd.Timedelta(days=val_days)\n",
    "        train = df_model[df_model[\"date\"] <= split_date]\n",
    "        val   = df_model[df_model[\"date\"] >  split_date]\n",
    "\n",
    "        X_train = train[cat_cols + base_cols]\n",
    "        y_train = train[\"demand\"].values\n",
    "        X_val = val[cat_cols + base_cols]\n",
    "        y_val = val[\"demand\"].values\n",
    "\n",
    "        # ensure categorical dtype\n",
    "        for c in cat_cols:\n",
    "            X_train[c] = X_train[c].astype(\"category\")\n",
    "            X_val[c] = X_val[c].astype(\"category\")\n",
    "\n",
    "        # default LGB params (you can tune)\n",
    "        if lgb_params is None:\n",
    "            lgb_params = dict(\n",
    "                n_estimators=2000,\n",
    "                learning_rate=0.03,\n",
    "                num_leaves=128,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            )\n",
    "\n",
    "        # mean model\n",
    "        self.model_mean = LGBMRegressor(**lgb_params)\n",
    "        self.model_mean.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=100),\n",
    "                log_evaluation(period=100),  # <-- replaces verbose=100\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        val_pred = self.model_mean.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, val_pred)\n",
    "\n",
    "        # quantile models\n",
    "        params_q = lgb_params.copy()\n",
    "        params_q[\"objective\"] = \"quantile\"\n",
    "        # q50\n",
    "        params_q50 = params_q.copy()\n",
    "        params_q50[\"alpha\"] = 0.5\n",
    "        self.model_q50 = LGBMRegressor(**params_q50)\n",
    "        self.model_q50.fit(X_train, y_train)\n",
    "\n",
    "        # q90\n",
    "        params_q90 = params_q.copy()\n",
    "        params_q90[\"alpha\"] = 0.9\n",
    "        self.model_q90 = LGBMRegressor(**params_q90)\n",
    "        self.model_q90.fit(X_train, y_train)\n",
    "\n",
    "        self.feature_cols = cat_cols + base_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.last_train_date = df[\"date\"].max()\n",
    "        self.train_metrics = {\"val_mae\": float(mae), \"split_date\": str(split_date.date())}\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"Training finished in {elapsed:.1f}s. Val MAE: {mae:.4f}\")\n",
    "\n",
    "        return self.train_metrics\n",
    "\n",
    "    def save(self, out_dir: str):\n",
    "        Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(self.model_mean, Path(out_dir) / \"model_mean.joblib\")\n",
    "        joblib.dump(self.model_q50, Path(out_dir) / \"model_q50.joblib\")\n",
    "        joblib.dump(self.model_q90, Path(out_dir) / \"model_q90.joblib\")\n",
    "        joblib.dump(self.feature_cols, Path(out_dir) / \"feature_cols.joblib\")\n",
    "        joblib.dump(self.cat_cols, Path(out_dir) / \"cat_cols.joblib\")\n",
    "        joblib.dump(self.train_metrics, Path(out_dir) / \"train_metrics.json\")\n",
    "        print(\"Saved models to\", out_dir)\n",
    "\n",
    "    def load(self, out_dir: str):\n",
    "        self.model_mean = joblib.load(Path(out_dir) / \"model_mean.joblib\")\n",
    "        self.model_q50 = joblib.load(Path(out_dir) / \"model_q50.joblib\")\n",
    "        self.model_q90 = joblib.load(Path(out_dir) / \"model_q90.joblib\")\n",
    "        self.feature_cols = joblib.load(Path(out_dir) / \"feature_cols.joblib\")\n",
    "        self.cat_cols = joblib.load(Path(out_dir) / \"cat_cols.joblib\")\n",
    "        self.train_metrics = joblib.load(Path(out_dir) / \"train_metrics.json\")\n",
    "        print(\"Loaded models from\", out_dir)\n",
    "\n",
    "    # -------------------------\n",
    "    # forecast_range: autoregressive multi-SKU forecasting\n",
    "    # -------------------------\n",
    "    def forecast_range(self, df_all: pd.DataFrame, sku_ids: List[str], start_date: str, end_date: str,\n",
    "                   use_mean_for_roll: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Faster forecast_range: only recomputes features for the requested sku_ids.\n",
    "        - df_all must be the full panel (calendar+price merged) covering requested dates.\n",
    "        - Returns DataFrame with ['date','id','fc_mean','fc_q50','fc_q90'] for requested SKUs.\n",
    "        \"\"\"\n",
    "        if self.model_mean is None:\n",
    "            raise RuntimeError(\"Model not trained. Call train(...) first.\")\n",
    "\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "\n",
    "        # Check calendar coverage\n",
    "        max_date = pd.to_datetime(df_all[\"date\"].max())\n",
    "        if end_date > max_date:\n",
    "            raise ValueError(f\"end_date {end_date.date()} beyond available calendar coverage {max_date.date()}.\")\n",
    "\n",
    "        # base working copy (full panel) — we'll update only rows for requested SKUs\n",
    "        work = df_all.copy()\n",
    "        work[\"date\"] = pd.to_datetime(work[\"date\"])\n",
    "        work = work.sort_values([\"id\", \"date\"]).reset_index(drop=False)  # preserve original index column 'index'\n",
    "        work = work.rename(columns={\"index\": \"orig_index\"})  # orig_index identifies row position in original df_all\n",
    "        work[\"demand\"] = work[\"demand\"].astype(float)\n",
    "\n",
    "        # ensure forecast columns exist\n",
    "        work[\"fc_mean\"] = np.nan\n",
    "        work[\"fc_q50\"] = np.nan\n",
    "        work[\"fc_q90\"] = np.nan\n",
    "\n",
    "        # prepare the smaller subset (only requested SKUs)\n",
    "        mask_sub = work[\"id\"].isin(sku_ids)\n",
    "        sub = work.loc[mask_sub].copy()   # this keeps orig_index values so we can write back easily\n",
    "        # keep orig_index as index for straightforward write-back\n",
    "        sub.set_index(\"orig_index\", inplace=True)\n",
    "\n",
    "        # cast categorical columns in sub for prediction speed\n",
    "        for c in self.cat_cols:\n",
    "            if c in sub.columns:\n",
    "                sub[c] = sub[c].astype(\"category\")\n",
    "\n",
    "        # pre-create fc columns in sub\n",
    "        sub[\"fc_mean\"] = np.nan\n",
    "        sub[\"fc_q50\"] = np.nan\n",
    "        sub[\"fc_q90\"] = np.nan\n",
    "\n",
    "        # iterate day by day only up to end_date; start from min date in work to ensure lags exist\n",
    "        all_days = pd.date_range(start=work[\"date\"].min(), end=end_date, freq=\"D\")\n",
    "        feature_cols = self.feature_cols\n",
    "        cat_cols = self.cat_cols\n",
    "\n",
    "        for day in all_days:\n",
    "            # recompute features for the sub-panel (only the SKUs we care about)\n",
    "            # add_panel_features uses groupby('id') and will compute lags/rollings correctly using current sub['demand']\n",
    "            sub = add_panel_features(sub.reset_index(drop=False).rename(columns={\"index\":\"orig_index\"})).set_index(\"orig_index\")\n",
    "            # note: after add_panel_features we re-index by orig_index again\n",
    "\n",
    "            # select rows for this day in sub\n",
    "            day_mask_idx = sub.index[sub[\"date\"] == day].tolist()\n",
    "            if len(day_mask_idx) == 0:\n",
    "                continue\n",
    "\n",
    "            X_day = sub.loc[day_mask_idx, feature_cols].copy()\n",
    "\n",
    "            # ensure categoricals match training\n",
    "            for c in cat_cols:\n",
    "                if c in X_day.columns:\n",
    "                    X_day[c] = X_day[c].astype(\"category\")\n",
    "\n",
    "            # predict\n",
    "            mean_hat = self.model_mean.predict(X_day)\n",
    "            q50_hat  = self.model_q50.predict(X_day)\n",
    "            q90_hat  = self.model_q90.predict(X_day)\n",
    "\n",
    "            # write to sub (these indices are orig_index values)\n",
    "            sub.loc[day_mask_idx, \"fc_mean\"] = mean_hat\n",
    "            sub.loc[day_mask_idx, \"fc_q50\"]  = q50_hat\n",
    "            sub.loc[day_mask_idx, \"fc_q90\"]  = q90_hat\n",
    "\n",
    "            # decide what to use to fill demand for subsequent days' lags\n",
    "            fill_vals = mean_hat if use_mean_for_roll else q50_hat\n",
    "            sub.loc[day_mask_idx, \"demand\"] = fill_vals\n",
    "\n",
    "            # also write these back into work so the \"global\" panel stays in sync (optional, but keeps single source)\n",
    "            # we use orig_index to map rows exactly\n",
    "            work.loc[day_mask_idx, [\"demand\", \"fc_mean\", \"fc_q50\", \"fc_q90\"]] = \\\n",
    "                sub.loc[day_mask_idx, [\"demand\", \"fc_mean\", \"fc_q50\", \"fc_q90\"]].values\n",
    "\n",
    "        # After loop, extract requested rows for the date range\n",
    "        out_mask = (work[\"id\"].isin(sku_ids)) & (work[\"date\"] >= start_date) & (work[\"date\"] <= end_date)\n",
    "        out = work.loc[out_mask, [\"date\", \"id\", \"fc_mean\", \"fc_q50\", \"fc_q90\"]].copy()\n",
    "        out = out.sort_values([\"id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "        # clip negatives (optional safety)\n",
    "        for c in [\"fc_mean\", \"fc_q50\", \"fc_q90\"]:\n",
    "            out[c] = out[c].clip(lower=0.0)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 6.69318\n",
      "[200]\tvalid_0's l2: 6.68694\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid_0's l2: 6.65348\n",
      "Training finished in 75.4s. Val MAE: 1.1289\n",
      "{'val_mae': 1.128860167111154, 'split_date': '2016-03-27'}\n",
      "Saved models to models/global_forecast_v1\n"
     ]
    }
   ],
   "source": [
    "# 1) load a subset (safe for laptop)\n",
    "df = load_m5_long(\"../data/raw/m5\", max_items=100)   # reduce max_items if low RAM\n",
    "\n",
    "# 2) train\n",
    "svc = GlobalForecastModel()\n",
    "metrics = svc.train(df, val_days=28)\n",
    "print(metrics)\n",
    "\n",
    "# 3) save\n",
    "svc.save(\"../models/global_forecast_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98edc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extend_panel_with_future(df_all: pd.DataFrame, m5_dir: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extends df_all to include future dates up to end_date for all SKUs already present in df_all.\n",
    "    Pulls future calendar rows from calendar.csv and prices from sell_prices.csv.\n",
    "\n",
    "    Assumes df_all already has these columns:\n",
    "      id, item_id, store_id, state_id, dept_id, cat_id, date, wm_yr_wk, price, and calendar fields.\n",
    "\n",
    "    Returns: extended dataframe including new future rows (demand is NaN for new rows).\n",
    "    \"\"\"\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    df_all = df_all.copy()\n",
    "    df_all[\"date\"] = pd.to_datetime(df_all[\"date\"])\n",
    "\n",
    "    current_max = df_all[\"date\"].max()\n",
    "    if end_date <= current_max:\n",
    "        return df_all\n",
    "\n",
    "    # Load calendar + prices\n",
    "    cal = pd.read_csv(f\"{m5_dir}/calendar.csv\")\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    prices = pd.read_csv(f\"{m5_dir}/sell_prices.csv\").rename(columns={\"sell_price\": \"price\"})\n",
    "\n",
    "    # Dates we need to add\n",
    "    future_cal = cal[(cal[\"date\"] > current_max) & (cal[\"date\"] <= end_date)].copy()\n",
    "    if future_cal.empty:\n",
    "        raise ValueError(f\"No calendar rows available after {current_max.date()} up to {end_date.date()}\")\n",
    "\n",
    "    # Unique SKU identities from current panel\n",
    "    sku_meta = df_all[[\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"]].drop_duplicates()\n",
    "\n",
    "    # Create cartesian product: (SKUs) x (future dates)\n",
    "    future_rows = sku_meta.merge(future_cal, how=\"cross\")\n",
    "\n",
    "    # Merge prices on (store_id, item_id, wm_yr_wk)\n",
    "    future_rows = future_rows.merge(prices, on=[\"store_id\",\"item_id\",\"wm_yr_wk\"], how=\"left\")\n",
    "\n",
    "    # Demand is unknown in future rows\n",
    "    future_rows[\"demand\"] = np.nan\n",
    "\n",
    "    # Align columns to df_all\n",
    "    missing_cols = [c for c in df_all.columns if c not in future_rows.columns]\n",
    "    for c in missing_cols:\n",
    "        future_rows[c] = np.nan\n",
    "\n",
    "    # Keep same column order as df_all\n",
    "    future_rows = future_rows[df_all.columns]\n",
    "\n",
    "    out = pd.concat([df_all, future_rows], ignore_index=True)\n",
    "    out = out.sort_values([\"id\",\"date\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a8203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>fc_mean</th>\n",
       "      <th>fc_q50</th>\n",
       "      <th>fc_q90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.467934</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>1.976701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-02</td>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.522983</td>\n",
       "      <td>0.104836</td>\n",
       "      <td>1.922672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.522983</td>\n",
       "      <td>0.085417</td>\n",
       "      <td>1.909235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.416908</td>\n",
       "      <td>0.024746</td>\n",
       "      <td>1.812157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.416908</td>\n",
       "      <td>0.006020</td>\n",
       "      <td>1.699342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                             id   fc_mean    fc_q50    fc_q90\n",
       "0 2016-04-01  HOBBIES_1_001_CA_1_validation  0.467934  0.006674  1.976701\n",
       "1 2016-04-02  HOBBIES_1_001_CA_1_validation  0.522983  0.104836  1.922672\n",
       "2 2016-04-03  HOBBIES_1_001_CA_1_validation  0.522983  0.085417  1.909235\n",
       "3 2016-04-04  HOBBIES_1_001_CA_1_validation  0.416908  0.024746  1.812157\n",
       "4 2016-04-05  HOBBIES_1_001_CA_1_validation  0.416908  0.006020  1.699342"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ext = extend_panel_with_future(df, m5_dir=\"../data/raw/m5\", end_date=\"2016-04-28\")\n",
    "\n",
    "sku_list = df_ext[\"id\"].drop_duplicates().head(10).tolist()\n",
    "fc = svc.forecast_range(df_ext, sku_list, start_date=\"2016-04-01\", end_date=\"2016-04-28\")\n",
    "fc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea3c3ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>demand</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id    d  demand       date  wm_yr_wk  ... month  year  event_name_1  \\\n",
       "0       CA  d_1     0.0 2011-01-29     11101  ...     1  2011           NaN   \n",
       "1       CA  d_2     0.0 2011-01-30     11101  ...     1  2011           NaN   \n",
       "2       CA  d_3     0.0 2011-01-31     11101  ...     1  2011           NaN   \n",
       "3       CA  d_4     0.0 2011-02-01     11101  ...     2  2011           NaN   \n",
       "4       CA  d_5     0.0 2011-02-02     11101  ...     2  2011           NaN   \n",
       "\n",
       "   event_type_1 event_name_2 event_type_2 snap_CA snap_TX  snap_WI  price  \n",
       "0           NaN          NaN          NaN       0       0        0    NaN  \n",
       "1           NaN          NaN          NaN       0       0        0    NaN  \n",
       "2           NaN          NaN          NaN       0       0        0    NaN  \n",
       "3           NaN          NaN          NaN       1       1        0    NaN  \n",
       "4           NaN          NaN          NaN       1       0        1    NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e91296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "M5_DIR = Path(\"../data/raw/m5\")   # change if needed\n",
    "\n",
    "def build_df_base(store_ids=(\"CA_1\",), max_ids=3000):\n",
    "    \"\"\"\n",
    "    Builds the base dataframe used by all agents.\n",
    "    \"\"\"\n",
    "    sales = pd.read_csv(M5_DIR / \"sales_train_validation.csv\")\n",
    "    calendar = pd.read_csv(M5_DIR / \"calendar.csv\")\n",
    "    prices = pd.read_csv(M5_DIR / \"sell_prices.csv\")\n",
    "\n",
    "    # Filter to selected stores (regions)\n",
    "    sales = sales[sales[\"store_id\"].isin(store_ids)].copy()\n",
    "\n",
    "    # Limit number of SKU-store series (for stability)\n",
    "    keep_ids = sales[\"id\"].drop_duplicates().head(max_ids)\n",
    "    sales = sales[sales[\"id\"].isin(keep_ids)].copy()\n",
    "\n",
    "    # Convert wide -> long\n",
    "    d_cols = [c for c in sales.columns if c.startswith(\"d_\")]\n",
    "    id_cols = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "\n",
    "    df = sales[id_cols + d_cols].melt(\n",
    "        id_vars=id_cols,\n",
    "        var_name=\"d\",\n",
    "        value_name=\"demand\"\n",
    "    )\n",
    "\n",
    "    # Merge calendar (date + week)\n",
    "    calendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n",
    "    df = df.merge(\n",
    "        calendar[[\"d\", \"date\", \"wm_yr_wk\"]],\n",
    "        on=\"d\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Merge prices\n",
    "    prices = prices.rename(columns={\"sell_price\": \"price\"})\n",
    "    df = df.merge(\n",
    "        prices[[\"store_id\", \"item_id\", \"wm_yr_wk\", \"price\"]],\n",
    "        on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Clean types\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[\"demand\"] = df[\"demand\"].astype(np.float32)\n",
    "    df[\"price\"] = df[\"price\"].astype(np.float32)\n",
    "\n",
    "    # Sort properly\n",
    "    df = df.sort_values([\"id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c799ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "from backend.forecast_agent import ForecastAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac547b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on our latest projections for the four-week period starting March 28th, we expect steady sales activity at the CA_1 store. Total demand across these items is estimated at 1,450 units. The majority of this volume is driven by a high-demand food item, which we expect customers to buy at a rate of about 51 units every day. In contrast, the hobby item moves much more slowly, with an average of only one sale per day. These figures suggest that while the food item requires constant attention, the hobby item will have very low turnover. Overall, our goal is to keep these products available for customers without keeping too much extra stock in the back.\n",
      "\n",
      "**Practical Planning Suggestion:**\n",
      "To maximize efficiency, schedule daily or every-other-day restocking for the high-volume food item to ensure it never runs out, while ordering the entire month's supply of the hobby item in a single small shipment to save on delivery efforts.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "# Set env var first in Colab:\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY\"\n",
    "\n",
    "client = genai.Client(api_key=\"\")\n",
    "\n",
    "agent = ForecastAgent(\n",
    "    model_dir=\"../models\",\n",
    "    m5_dir=\"../data/raw/m5\",\n",
    "    llm_client=client   # Gemini\n",
    ")\n",
    "\n",
    "out = agent.forecast(\n",
    "    item_ids=[\"FOODS_3_090\", \"HOBBIES_1_001\"],\n",
    "    start_date=\"2016-03-28\",\n",
    "    end_date=\"2016-04-24\"\n",
    ")\n",
    "\n",
    "print(out[\"llm_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e010d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start_date', 'end_date', 'results', 'llm_summary', 'model_metrics'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f79c2106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'item_id': 'FOODS_3_090',\n",
       "  'store_id': 'CA_1',\n",
       "  'series_id': 'FOODS_3_090_CA_1_validation',\n",
       "  'daily_forecast':            date  forecast\n",
       "  1885 2016-03-28      25.0\n",
       "  1886 2016-03-29      47.0\n",
       "  1887 2016-03-30      23.0\n",
       "  1888 2016-03-31      21.0\n",
       "  1889 2016-04-01      72.0\n",
       "  1890 2016-04-02      58.0\n",
       "  1891 2016-04-03      54.0\n",
       "  1892 2016-04-04      42.0\n",
       "  1893 2016-04-05      29.0\n",
       "  1894 2016-04-06      53.0\n",
       "  1895 2016-04-07      39.0\n",
       "  1896 2016-04-08      42.0\n",
       "  1897 2016-04-09      83.0\n",
       "  1898 2016-04-10      83.0\n",
       "  1899 2016-04-11      48.0\n",
       "  1900 2016-04-12      26.0\n",
       "  1901 2016-04-13      41.0\n",
       "  1902 2016-04-14      44.0\n",
       "  1903 2016-04-15      47.0\n",
       "  1904 2016-04-16      82.0\n",
       "  1905 2016-04-17      83.0\n",
       "  1906 2016-04-18      30.0\n",
       "  1907 2016-04-19      45.0\n",
       "  1908 2016-04-20      29.0\n",
       "  1909 2016-04-21      53.0\n",
       "  1910 2016-04-22      87.0\n",
       "  1911 2016-04-23      95.0\n",
       "  1912 2016-04-24      42.0,\n",
       "  'total_units': 1423.0},\n",
       " {'item_id': 'HOBBIES_1_001',\n",
       "  'store_id': 'CA_1',\n",
       "  'series_id': 'HOBBIES_1_001_CA_1_validation',\n",
       "  'daily_forecast':            date  forecast\n",
       "  1885 2016-03-28       1.0\n",
       "  1886 2016-03-29       0.0\n",
       "  1887 2016-03-30       0.0\n",
       "  1888 2016-03-31       0.0\n",
       "  1889 2016-04-01       0.0\n",
       "  1890 2016-04-02       0.0\n",
       "  1891 2016-04-03       1.0\n",
       "  1892 2016-04-04       0.0\n",
       "  1893 2016-04-05       4.0\n",
       "  1894 2016-04-06       2.0\n",
       "  1895 2016-04-07       3.0\n",
       "  1896 2016-04-08       0.0\n",
       "  1897 2016-04-09       1.0\n",
       "  1898 2016-04-10       2.0\n",
       "  1899 2016-04-11       0.0\n",
       "  1900 2016-04-12       0.0\n",
       "  1901 2016-04-13       0.0\n",
       "  1902 2016-04-14       1.0\n",
       "  1903 2016-04-15       1.0\n",
       "  1904 2016-04-16       3.0\n",
       "  1905 2016-04-17       0.0\n",
       "  1906 2016-04-18       1.0\n",
       "  1907 2016-04-19       1.0\n",
       "  1908 2016-04-20       1.0\n",
       "  1909 2016-04-21       3.0\n",
       "  1910 2016-04-22       0.0\n",
       "  1911 2016-04-23       1.0\n",
       "  1912 2016-04-24       1.0,\n",
       "  'total_units': 27.0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60adc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.simulation_agent import SimulationAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cbac8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = out['results'][0][\"daily_forecast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24958797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id': 'FOODS_3_090',\n",
       " 'store_id': 'CA_1',\n",
       " 'policy': {'s': 80, 'Q': 120},\n",
       " 'lead_time_days': 2,\n",
       " 'initial_inventory': 80,\n",
       " 'results': {'expected_fill_rate': 0.8617199243804855,\n",
       "  'stockout_probability': 1.0,\n",
       "  'avg_stockout_days': 5.592,\n",
       "  'expected_lost_units': 198.29348497883825,\n",
       "  'avg_inventory': 48.05153724327553,\n",
       "  'avg_orders': 10.708},\n",
       " 'scenario_summary': {'mean_total_demand': 1423.6870711196088,\n",
       "  'p95_total_demand': 1539.8132012016497}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_agent = SimulationAgent(\n",
    "    n_simulations=500,\n",
    "    demand_cv=0.25,\n",
    "    lead_time_days=2\n",
    ")\n",
    "\n",
    "sim_result = sim_agent.simulate(\n",
    "    forecast_df=forecast_df,\n",
    "    item_id=\"FOODS_3_090\",\n",
    "    store_id=\"CA_1\",\n",
    "    s=80,\n",
    "    Q=120\n",
    ")\n",
    "\n",
    "sim_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da71c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_products = [\"FOODS_3_090\", \"HOBBIES_1_001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d2cd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(selected_products)):\n",
    "    sim = sim_agent.simulate(\n",
    "        forecast_df=out['results'][i][\"daily_forecast\"],\n",
    "        item_id=selected_products[i],\n",
    "        store_id=\"CA_1\",\n",
    "        s=100,\n",
    "        Q=350\n",
    "    )\n",
    "    results.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "998796a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'item_id': 'FOODS_3_090',\n",
       "  'store_id': 'CA_1',\n",
       "  'policy': {'s': 100, 'Q': 350},\n",
       "  'lead_time_days': 2,\n",
       "  'initial_inventory': 201,\n",
       "  'results': {'expected_fill_rate': 0.9677501310317528,\n",
       "   'stockout_probability': 0.736,\n",
       "   'avg_stockout_days': 1.438,\n",
       "   'expected_lost_units': 45.19710978373548,\n",
       "   'avg_inventory': 161.8198035413653},\n",
       "  'scenario_summary': {'mean_total_demand': 1423.2762855293638,\n",
       "   'p95_total_demand': 1544.187539081274}},\n",
       " {'item_id': 'HOBBIES_1_001',\n",
       "  'store_id': 'CA_1',\n",
       "  'policy': {'s': 100, 'Q': 350},\n",
       "  'lead_time_days': 2,\n",
       "  'initial_inventory': 101,\n",
       "  'results': {'expected_fill_rate': 1.0,\n",
       "   'stockout_probability': 0.0,\n",
       "   'avg_stockout_days': 0.0,\n",
       "   'expected_lost_units': 0.0,\n",
       "   'avg_inventory': 391.55832896136764},\n",
       "  'scenario_summary': {'mean_total_demand': 32.69605548435732,\n",
       "   'p95_total_demand': 39.61397839618095}}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8104528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5739000, 11)\n"
     ]
    }
   ],
   "source": [
    "df_base = build_df_base(\n",
    "    store_ids=(\"CA_1\", \"TX_1\", \"WI_1\"),  # all regions\n",
    "    max_ids=3000                         # safe size\n",
    ")\n",
    "\n",
    "print(df_base.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f28324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price = (\n",
    "    df_base[\n",
    "        (df_base[\"item_id\"] == \"FOODS_3_090\") &\n",
    "        (df_base[\"store_id\"] == \"CA_1\")\n",
    "    ][\"price\"].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e507b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.optimization_agent import OptimizationAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1ffdce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id': 'FOODS_3_090',\n",
       " 'store_id': 'CA_1',\n",
       " 'policy': {'s': 80, 'Q': 120},\n",
       " 'lead_time_days': 2,\n",
       " 'initial_inventory': 80,\n",
       " 'results': {'expected_fill_rate': 0.8617199243804855,\n",
       "  'stockout_probability': 1.0,\n",
       "  'avg_stockout_days': 5.592,\n",
       "  'expected_lost_units': 198.29348497883825,\n",
       "  'avg_inventory': 48.05153724327553,\n",
       "  'avg_orders': 10.708},\n",
       " 'scenario_summary': {'mean_total_demand': 1423.6870711196088,\n",
       "  'p95_total_demand': 1539.8132012016497}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_agent = SimulationAgent(\n",
    "    n_simulations=500,\n",
    "    demand_cv=0.25,\n",
    "    lead_time_days=2\n",
    ")\n",
    "\n",
    "sim_result = sim_agent.simulate(\n",
    "    forecast_df=out['results'][0][\"daily_forecast\"],\n",
    "    item_id=\"FOODS_3_090\",\n",
    "    store_id=\"CA_1\",\n",
    "    s=80,\n",
    "    Q=120\n",
    ")\n",
    "\n",
    "sim_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81649dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_agent = OptimizationAgent(\n",
    "    simulation_agent=sim_agent,\n",
    "    holding_cost_rate=0.25,        # 25% annually\n",
    "    order_cost=20.0,               # $20 per order\n",
    "    stockout_cost_multiplier=3.0,  # lost sales penalty\n",
    "    target_fill_rate=0.95\n",
    ")\n",
    "\n",
    "# Candidate policies to try\n",
    "s_candidates = range(50, 201, 25)\n",
    "Q_candidates = range(100, 401, 50)\n",
    "\n",
    "opt_result = opt_agent.optimize(\n",
    "    forecast_df=out['results'][0][\"daily_forecast\"],\n",
    "    item_id=\"FOODS_3_090\",\n",
    "    store_id=\"CA_1\",\n",
    "    avg_price=avg_price,\n",
    "    s_candidates=s_candidates,\n",
    "    Q_candidates=Q_candidates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "290b00e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id': 'FOODS_3_090',\n",
       " 'store_id': 'CA_1',\n",
       " 's': 200,\n",
       " 'Q': 400,\n",
       " 'fill_rate': 0.9991917685612025,\n",
       " 'total_cost': 92.85790252685547,\n",
       " 'holding_cost': 7.7258453369140625,\n",
       " 'ordering_cost': 80.16,\n",
       " 'stockout_cost': 4.972055435180664}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_result[\"best_policy\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
